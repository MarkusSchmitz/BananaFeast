{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, a Deep-Q-Network is composed in order to solve the following project challenge;\n",
    "[Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "#### Set the \"train\" parameter to False to see a smart agent. Set it True to train the Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False # set true to train agent and false to watch the trained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Imports\n",
    "\n",
    "We start by importing some useful packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment # Runs the training environment\n",
    "import numpy as np # General Purpose Data\n",
    "from dqn_agent_new import Agent # The agent class that handles actions and training\n",
    "from collections import deque # Used for bundling and extracting data\n",
    "import matplotlib.pyplot as plt # used for plotting learning results\n",
    "import torch # Saving and loading Nets\n",
    "import time # for time estimation etc.\n",
    "from sklearn.model_selection import ParameterGrid # The class that handles grid optimization\n",
    "import seaborn as sns # Pretty Plotting\n",
    "sns.set\n",
    "sns.set_palette(\"husl\")\n",
    "import pandas as pd\n",
    "import statistics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is made to run on a linux x64 machine. For other training images please refer to the Udacity Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup the environment and set parameters\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# create environment and load API\n",
    "env = UnityEnvironment(file_name=\"../BananaFeast/Banana_Linux/Banana.x86_64\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "# 3. Setting the functions\n",
    "The dqn function trains an agent using a Double-Deep-Q-Network.\n",
    "The Agent is initialized inside the function in order to enable multiple successive training runs.\n",
    "The parameters of training can all be adjusted with the dqn function and are set, by default, to the best found solution.\n",
    "\n",
    "\n",
    "We also have a function for plotting the Agents results and learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dqn(n_episodes=1500, max_t=1500, eps_start=1.0, eps_end=0.01, eps_decay=0.99, BUFFER_SIZE = int(1e5), \n",
    "        BATCH_SIZE = 64, GAMMA = 0.99, TAU = 1e-3, LR = 5e-4, UPDATE_INTERVAL = 20, model = True):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int):       maximum number of training episodes\n",
    "        max_t (int):            maximum number of timesteps per episode\n",
    "        eps_start (float):      starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float):        minimum value of epsilon\n",
    "        eps_decay (float):      multiplicative factor (per episode) for decreasing epsilon\n",
    "        BUFFER_SIZE (int):      Size of Memory Buffer\n",
    "        BATCH_SIZE (int):       Size of memory batch trained on\n",
    "        GAMMA (float):          discount factor for future rewards\n",
    "        TAU (float):            for soft update of target parameters\n",
    "        LR (float):             learning rate \n",
    "        UPDATE_INTERVAL (int):  how often to update the network\n",
    "        EPSILON (float):        Exploration Rate\n",
    "        model (boolean):        use small (True) or big (False) Net       \n",
    "        \n",
    "    \"\"\"\n",
    "    # 1. Initiate Agent class and stores for scores \n",
    "    agent = Agent(BUFFER_SIZE = BUFFER_SIZE, BATCH_SIZE = BATCH_SIZE, GAMMA = GAMMA, \n",
    "        TAU = TAU, LR = LR, UPDATE_INTERVAL = UPDATE_INTERVAL ,state_size=37, action_size=4, seed=42, EPSILON=eps_start, model = model)\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    \n",
    "    # initialize epsilon for training\n",
    "    eps = eps_start\n",
    "    \n",
    "    # iterate over the amount of training episodes\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        #reset env, score and get first state value\n",
    "        env_info = env.reset(train_mode=True)[brain_name]   #reset env\n",
    "        state = env_info.vector_observations[0]          \n",
    "        score = 0\n",
    "        \n",
    "        # act inside the environment until maxtime or the env is \"done\"\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            action = agent.get_action(state)        # get next action based on state\n",
    "            env_info = env.step(action)[brain_name] # apply action to environment\n",
    "            \n",
    "            next_state, reward, done = env_info.vector_observations[0], env_info.rewards[0], env_info.local_done[0] # update values to new env state\n",
    "            agent.step(state, action, reward, next_state, done) # record values and train if finished cycle\n",
    "            \n",
    "            state = next_state #update state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        agent.EPSILON = eps               # update new epsilon to agent\n",
    "        \n",
    "        if agent.scoremax < np.mean(scores_window): \n",
    "            agent.scoremax = np.mean(scores_window) # if agents performance increased, update own threshold \n",
    "        exp, lvl, maxscore = agent.get_stats()      # get agents current stats\n",
    "        \n",
    "        # Output training results for evaluation\n",
    "        print('\\rEpisode {}\\tAvg. Score: {:.2f}\\tExperience: {}\\tcurrent Level: {}\\tMax Score: {:.2f}'.format(i_episode, np.mean(scores_window), exp, lvl, maxscore), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            \n",
    "    # if the agent reaches minimum score, save the agents network    \n",
    "    if np.mean(scores_window)>=13.0:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "        torch.save(agent.localNet.state_dict(), 'weights.tnn')\n",
    "    print(\"\\nfinished training\")\n",
    "    \n",
    "    return scores, agent.scoremax\n",
    "\n",
    "\n",
    "def plot_res(score):\n",
    "    \"\"\"\n",
    "    DQN result plotting\n",
    "    \n",
    "    plots the learning results of agent in a graph\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        score (list<int>): list of past training results    \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # write results in dataframe\n",
    "    data = pd.DataFrame({\"episode\": np.arange(len(score)), \"score\" : score})\n",
    "    data.episode = (data.episode // 20) + 1 \n",
    "    \n",
    "    #plot learning graph\n",
    "    fig, ax = plt.subplots(figsize=(25,20))\n",
    "    ax = sns.lineplot(x = \"episode\", y = \"score\", data = data)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "# 4. Train the Agent\n",
    "\n",
    "Before training, we create a parametergrid for the training cycles.\n",
    "This way we are able to iterate through different parameter seetings and hone done to the best performing model.\n",
    "\n",
    "Next, the model is trained. The scores are saved to csv files in order to document the training.\n",
    "After the Agent has trained for the set amount of episodes, the training process is displayed in a graph. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of training configurations is: 1\n"
     ]
    }
   ],
   "source": [
    "# Create Paramater grid for optimization\n",
    "# add values to the parameter list to train in different configs\n",
    "grid = {\n",
    "    \"n_episodes\" : [1500],\n",
    "    \"BUFFER_SIZE\" : [int(1e5)],\n",
    "    \"BATCH_SIZE\" : [64],\n",
    "    \"GAMMA\" : [0.99],\n",
    "    \"TAU\" : [1e-3],\n",
    "    \"LR\" : [5e-4],\n",
    "    \"UPDATE_INTERVAL\" : [20],\n",
    "    \"model\" : [True],\n",
    "    \"eps_decay\" : [0.99]\n",
    "}\n",
    "\n",
    "print(\"The amount of training configurations is:\",len(list(ParameterGrid(grid))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    \n",
    "    #set stores for training results\n",
    "    scores = []\n",
    "    scoremaxima = []\n",
    "    scoreparam = []\n",
    "    \n",
    "    i = 1 # run counter\n",
    "    \n",
    "    # iterate over all parameter configs\n",
    "    for params in ParameterGrid(grid):\n",
    "        print(\"--------------------------------------------------------------------------------------------------\")\n",
    "        print(\"run\\t\", i)\n",
    "        print(params)\n",
    "        \n",
    "        # run the agent with the parameter config\n",
    "        kwargs = params\n",
    "        %time score, scoremax = dqn(**kwargs)\n",
    "\n",
    "        # save parameters to file\n",
    "        scoreparam.append(list(params.values()).append(scoremax))\n",
    "        pd.DataFrame(scoreparam).to_csv(\"results.csv\")\n",
    "    \n",
    "        # plot the results of training\n",
    "        plot_res(score)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "# 5. Watch the agent\n",
    "\n",
    "Here you can watch the smart Agent act inside the env and collect bananas.\n",
    "The Agent should be able to get around 15 points on average. \n",
    "\n",
    "Depending on random generation the agent may achieve as low as 8 or as high as 21 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 16.00"
     ]
    }
   ],
   "source": [
    "# Watch the smart agent\n",
    "if train == False:\n",
    "    \n",
    "    n_episodes = 1 # number of runs\n",
    "    max_t = 1000   # max number of steps to solve env \n",
    "    \n",
    "    # load the trained agent\n",
    "    trained_agent = Agent(state_size=37, action_size=4, seed=42,EPSILON=0)\n",
    "    trained_agent.localNet.load_state_dict(torch.load('weights.tnn'))\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    # iterate over runs\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        # get environment infos and set first state\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]          \n",
    "        \n",
    "        # run until finished or max steps\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            action = trained_agent.get_action(state) # get next action based on state\n",
    "            env_info = env.step(action)[brain_name]  # act in env\n",
    "            \n",
    "            # update environment info\n",
    "            next_state, reward, done = env_info.vector_observations[0], env_info.rewards[0], env_info.local_done[0]\n",
    "            state = next_state\n",
    "            score = score + reward\n",
    "            \n",
    "            time.sleep(0.016) # makes actions visable for humans (~60 fps)\n",
    "            print('\\rScore: {:.2f}'.format(score), end=\"\")\n",
    "            if done:\n",
    "                break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when finished, close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
